{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "\n",
    "!pip install langchain_community\n",
    "!pip install langchain\n",
    "!pip install langchain_experimental\n",
    "!pip install langchain_chroma\n",
    "!pip install langchain_openai\n",
    "!pip install langchain_huggingface\n",
    "\n",
    "# !pip install polyglot\n",
    "# !pip install pyicu\n",
    "# polyglot download embeddings2.ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ODQA'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# langsmith로 로그 추적 가능하니 참고할 것\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"ODQA\"\n",
    "os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_1576087/1189191765.py:8: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from helper import EmbeddingHelper, KiwiBM25Retriever\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['source', 'seq_num']\n",
      "\n",
      "[examples]\n",
      "source  : /data/ephemeral/home/donghyeok/data/wikipedia_documents.json\n",
      "seq_num : 1\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
    "from helper import EmbeddingHelper, KiwiBM25Retriever\n",
    "#Custom_JSONLoader\n",
    "\n",
    "\n",
    "############################ 문서 loading(pdf 기준) ############################\n",
    "###### 문서 불러옴\n",
    "\n",
    "FILE_PATH = \"/data/ephemeral/home/donghyeok/data/wikipedia_documents.json\"\n",
    "helper = EmbeddingHelper()  # cache embedding (한 번 embedding 한거 저장용)\n",
    "loader = JSONLoader(\n",
    "    file_path=FILE_PATH, \n",
    "    jq_schema=\".[].text\",  \n",
    "    text_content=False\n",
    ")\n",
    "# 문서 로딩 및 확인\n",
    "docs = loader.load()\n",
    "helper.show_metadata(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#확인용\n",
    "# import inspect\n",
    "\n",
    "# print(inspect.getsource(JSONLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ embedding 모델 ############################\n",
    "###### embedding 할 모델 선택\n",
    "from transformers import AutoModel\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "\n",
    "# emb_model = HuggingFaceEndpointEmbeddings(\n",
    "#     model=model_name,\n",
    "#     task=\"feature-extraction\",\n",
    "#     huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],\n",
    "# )\n",
    "\n",
    "emb_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# embedding 내역 캐싱. helper.py 참고할 것. (default로 아마 project 폴더 내 cache 폴더 만들어서 저장될 것)\n",
    "embeddings = helper.cache_embedding(emb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ 문서 split ############################\n",
    "###### 불러온 문서 chunking (smemantic chunker는 방법 상 embedding이 필요함)\n",
    "\n",
    "# text splitter  (recursive, semantic 참고해서 둘 중 하나 써보길 지금은 그냥 돌게 해둠)\n",
    "text_splitter_rc = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "text_splitter_sm = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=80,\n",
    ")\n",
    "\n",
    "selected_splitter = text_splitter_rc\n",
    "\n",
    "###### 둘 다써보고싶으면 확인해보라고 남겨둠. ######\n",
    "# splitters = {\"Recursive\": text_splitter_rc, \"Semantic\": text_splitter_sm}\n",
    "# split_documents = {}\n",
    "\n",
    "# for n, s in splitters.items():\n",
    "#     split_documents[n] = s.split_documents(docs)\n",
    "\n",
    "\n",
    "# for method, docs in split_documents.items():\n",
    "#     print(f\"====== {method} Splitter Results =====\")\n",
    "#     for i, doc in enumerate(docs[:15]):\n",
    "#         print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n",
    "#         print(doc.page_content)\n",
    "#         print(\"===\" * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1335 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start1\n",
      "end1\n"
     ]
    }
   ],
   "source": [
    "############################ ensemble retriever 생성 ############################\n",
    "\n",
    "\n",
    "# if selected_splitter == \"recursive\":\n",
    "#     doc_splits = text_splitter_rc.split_documents(docs)\n",
    "# elif selected_splitter == \"semantic\":\n",
    "#     doc_splits = text_splitter_sm.split_documents(docs)\n",
    "\n",
    "\n",
    "\n",
    "doc_splits = text_splitter_rc.split_documents(docs)\n",
    "\n",
    "print(\"start1\")\n",
    "kiwi_bm25 = KiwiBM25Retriever.from_documents(doc_splits)\n",
    "print(\"end1\")\n",
    "\n",
    "\n",
    "# print(\"start2\")\n",
    "\n",
    "# def batch_documents(docs, batch_size=5):\n",
    "#     \"\"\"문서를 batch_size 크기로 나눕니다.\"\"\"\n",
    "#     for i in range(0, len(docs), batch_size):\n",
    "#         yield docs[i:i + batch_size]\n",
    "\n",
    "# # documents를 배치로 나눠서 Chroma에 전송\n",
    "# for doc_batch in batch_documents(doc_splits, batch_size=5):\n",
    "#     chroma_mmr = Chroma.from_documents(\n",
    "#         documents=doc_batch,\n",
    "#         collection_name=\"rag-chroma\",\n",
    "#         embedding=embeddings\n",
    "#     ).as_retriever(search_type=\"mmr\")\n",
    "\n",
    "# print(\"end2\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################ 여러개 시험해보고 싶을 때 참고할 것 ###########################\n",
    "# chroma_sim = Chroma.from_documents(\n",
    "#     documents=doc_splits,\n",
    "#     collection_name=\"rag-chroma\",\n",
    "#     embedding=embeddings,\n",
    "# ).as_retriever(search_type=\"mmr\")\n",
    "\n",
    "# chroma_mmr = Chroma.from_documents(\n",
    "#     documents=doc_splits,\n",
    "#     collection_name=\"rag-chroma\",\n",
    "#     embedding=embeddings,\n",
    "# ).as_retriever(search_type=\"mmr\")\n",
    "\n",
    "# bm25_chroma_73 = EnsembleRetriever(\n",
    "#     retrievers=[kiwi_bm25, chroma_mmr],\n",
    "#     weights=[0.7, 0.3],\n",
    "#     search_type=\"mmr\",\n",
    "# )\n",
    "\n",
    "# bm25_chroma_37 = EnsembleRetriever(\n",
    "#     retrievers=[kiwi_bm25, chroma_mmr],\n",
    "#     weights=[0.3, 0.7],\n",
    "#     search_type=\"mmr\",\n",
    "# )\n",
    "\n",
    "# bm25_chroma_55 = EnsembleRetriever(\n",
    "#     retrievers=[kiwi_bm25, chroma_mmr],\n",
    "#     weights=[0.5, 0.5],\n",
    "#     search_type=\"mmr\",\n",
    "# )\n",
    "\n",
    "# retrievers = {\n",
    "#     \"kiwi_bm25\": kiwi_bm25,\n",
    "#     \"chroma_sim\": chroma_sim,\n",
    "#     \"chroma_mmr\": chroma_mmr,\n",
    "#     \"bm25_chroma_37\": bm25_chroma_37,\n",
    "#     \"bm25_chroma_73\": bm25_chroma_73,\n",
    "#     \"bm25_chroma_55\": bm25_chroma_55,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = EnsembleRetriever(\n",
    "    retrievers=[kiwi_bm25, kiwi_bm25],\n",
    "    weights=[0.5, 0.5],\n",
    "    search_type=\"mmr\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ensemble Retriever]\n",
      "Content: 4대강 정비 사업\n",
      "2010년 8월 17일, 당일 방송예정이던 \"4대강 수심 6m ··· 누가 밀어 붙였나?\"에 국토해양부 4대강살리기 추진본부는 MBC PD 수첩이 방송을 앞두고 사전배포한 보도자료가 명백한 허위사실인데도 신문과 방송, 인터넷 등을 통해 급속하게 확산되고 있어 방송금지 가처분 신청을 서울남부지방법원에 냈다고 밝혔다.  그러나 서울남부지법 민사51부(양재영 부장판사)는 국토해양부가 낸 방송금지 가처분 신청을 기각했다. 재판부는 결정문에서, \"기록만으로는 방송예정인 프로그램의 내용이 명백히 진실이 아니고 방송 목적이 공공의 이익에 부합하지 않는다고 보기 어렵다. 또한 방송이 이뤄진다고 해서 신청인에게 중대하고 회복하기 어려운 손해를 입힐 우려가 있다는 점에 대한 소명이 부족하다\"라고 밝혔다.  'PD수첩' 제작진에 주장에 의하면 김재철 문화방송 사장은 임원회의에서 사규위반을 이유로 처음에 방송보류를 지시했다고 한다.  그러나 이 방송은 2010년 8월 24일에 방송된다.\n",
      "\n",
      "Content: 이철희 장영자 어음 사기 사건은 1982년 5월 4일, 사채시장의 '큰손'으로 불리던 장영자와 남편 이철희가 어음사기 혐의로 검찰에 구속되면서 일어난 대규모 어음 사기사건이다. \\n\\n장영자는 국회의원과 국가안전기획부 차장을 지낸 이철희를 내세워 미모와 화려한 언변으로 고위층과 긴밀한 관계를 과시한 후 자금난을 겪고 있는 기업들에 자금지원 대가로 2배에서 최고 9배짜리 어음을 받아 이를 사채시장에 유통시키고 돈을 착복했다. 어음과 담보조로 받은 견질어음을 몽땅 시중에서 할인한 후 다시 굴리는 수법으로 6400억 원의 어음을 시중에 유통시켜 1400여 억 원을 사기로 취했다. \\n\\n이 사건으로 인해 소위 ‘장영자 후폭풍’이라는 정계·경제계는 물론 사회 각 분야에 엄청난 파문이 몰아쳤는데, 특히 경제계에 엄청난 파문이 일었다. 장영자는 ‘경제는 유통이다’라는 유명한 말로 항변했지만 어음이 한 바퀴 돌았을 때 어음을 발행한 기업들이 부도를 내고 무너지기 시작했다. 이 사건으로 한국에서 처음으로 금융실명제 논의가 본격화되기 시작하였다. 검찰 수사 결과 이철희·장영자 뒤에는 장영자의 형부이자 전두환의 처삼촌인 이규광이 버티고 있었고, 이 사건으로 두 사람은 물론 은행장 2명과 공영토건, 일신제강 등 내로라하는 기업인 등 모두 32명이 구속됐다. \\n\\n장영자·이철희는 모두 15년의 징역형을 선고받고 복역 중 이철희가 먼저 가석방된 뒤, 장영자는 복역 10년 만인 1992년 3월 역시 가석방으로 풀려났다. \\n\\n그러나 장영자는 1994년 다시 100억 원대의 어음 사기사건으로 구속되어 복역하였고, 2001년 5월에도 220억 원대의 구권화폐 사기 행각을 벌인 혐의로 구속됨으로써 세 번째로 복역하게 되었다.\n",
      "\n",
      "Content: 1959년 태어난 조영철은 경북고등학교와 서울대학교 법학과를 졸업하고 제25회 사법시험 합격했다. 제15기 사법연수원과 군 법무관을 마치고 1989년 대구지방법원 판사에 임용되었다. 이후 대구지방법원 김천지원, 수원지방법원, 서울지방법원, 서울고등법원으로 전보되어 판사를 하다가 대법원 재판연구관을 거쳐 부장판사에 승진하여 대구지방법원, 서울중앙지방법원, 수원지방법원, 광주고등법원, 서울고등법원에서 부장판사로 재판장을 하였으며 2014년에는 서울중앙지방법원 민사수석부장판사를 하다가 2015년 2월 법원장으로 승진하여 의정부지방법원 법원장에 취임하였다. 의정부지방법원에서 법원장으로 2년동안 재직하다가 서울고등법원 부장판사로 복귀하여 재판을 하였다.\\n\\n공사계약에서 리베이트 약정은 무효, 군부대에서 벌목작업에 참여했다 사고로 숨진 병사를 월북자로 알리고 가족을 수십 년간 감시한 사안에 대해 국가의 손해배상책임을 인정, 대학교 동창회장이 일정 금액을 발전기금으로 내도록 한 동창회 선거규정 무효라는 판결을 했었던 조영철은 박근혜 정부에서 국정농단 혐의를 받은 최순실 측에서 \"이화여자대학교 학사비리 재판을 하면서 1심과 같이 징역3년 등을 선고한 바가 있는 재판장 조영철이 재판을 불공정하게 할 우려가 있어 이에 대한 기피를 신청한다\"며 재판부 기피 신청을 했다. \\n\\n오패산 터널 인근에서 사제 총기를 난사해 경찰관을 숨지게 한 혐의로 1심에서 무기징역을 선고받은 성병대가 법정에서 재판장인 조영철에게 \"이번 법관 인사로 좌·우배석 판사님들은 다 바뀌었는데 왜 재판장님만 안 바뀌는 건가요. 재판장님이 경찰의 청탁을 받았다는 제 이야기가 맞아떨어지는 것 아닙니까?\"고 항의하면서 자신의 사건을 \"다른 재판부로 보내달라\"고 했다. \\n\\n법원장으로 있다가 고등법원 부장판사로 재판부를 근무를 했던 조영철은 2019년 1월 28일에 있었던 법원인사에서 대구고등법원 법원장에 임명되었다.\n",
      "\n",
      "Content: 구·효고 전기 궤도에 유래하는 효고 - 아카시간의 궤도와 구·고베 히메지 전기 철도에 유래하는 아카시 역앞(현재의 산요아카시) - 히메지 역앞(현재의 산요히메지) 사이의 철도가 노선의 모체가 되고 있다.\n",
      "\n",
      "선행해 개통한 효고 전기 궤도(이하 '효철')는 개업 이래 경영이 불안정한 면이 있어, 서아카시로의 연장 시에 별도의 회사인 고베 히메지 전기 철도(이하 '고베 철도')를 설립하는 것으로 리스크 회피를 도모했다. 그러나 반슈 철도(현재의 JR가코가와 선의 전신)의 경영진에 의한 효철의 적대적 매수에 의해 반슈 철도 산하가 된 효철과 구 효철파의 고베 철도는 소원하게 되었던 시기도 있었다. 그러한 혼란 수습과 아울러 자사의 매전처 확보를 노린 전쟁 전의 대기업 전력회사·우지강전기(간사이 전력의 전신의 하나)가 양 회사를 병합 해 자사의 철도 사업 부문으로 했다. 후에 우지강전기가 본업에의 추출에 의해 철도 부문을 분리했을 때에 새롭게 설립된 것이 현재의 산요 전기 철도이다.\n",
      "\n",
      "합병전 2회사의 노선 규격의 상위 등 곤란한 조건을 극복해 직통 운전을 실현해,현청 소재지의 고베와 하리마의 중심지인 히메지를 직결하는 도시간 연락 노선으로 성장했다. 그리고 1940년에는 연선에서의 공원 수송을 목적으로 지선의 아보시 선을 개통시켰다.\n",
      "\n",
      "전후 운수성의 63형 전철 도입을 기회로 고속 철도로의 탈피를 도모한다. 또 가와사키 차량→가와사키 중공업과 협력해,850형 로망스카의 대형차량의 채용이나 2000계등의 선진적인 설계의 전철의 투입 등 적극성을 발휘, 특히 1962년에는 고속 전동차로서 일본 최초의 올 알루미늄차를 도입하고 있다. 또 전후도 오랜 세월에 걸쳐서 고베시안에 있어서의 병용 궤도 구간(노면 주행 구간)이 잔존해 운행의 어려움이 되고 있었지만,1968년에는 고베 고속 철도의 지하선 직통을 개시해 병용 궤도를 폐지하는 것과 동시에 게이한신 급행 전철(현·한큐 전철) 및 한신 전기 철도로부터의 직통열차가 노선 연장하게 되어 산노미야·오사카 방면과의 관계를 강하게 할 수 있었다.\n",
      "\n",
      "1970년대에 들어가면, 연선의 하리마 임해 공업 지대에서 제철업의 침체 경향이 표면화했다. 아보시 선의 건설 이유였던 신일본제철(←후지 제철←일본제철)히로하타 제철소의 용광로 휴지와 거기에 따르는 대규모 정리해고 등은 산요 전기 철도의 경영에도 큰 타격을 주었다. 연선에는 교외 지역이 많아서 모터라제이션의 영향도 심각하게 다가왔다. 게다가 고베 시내 및 아카시 역 부근까지 서일본 여객철도(JR서일본)의 산요 본선(JR 고베 선)과 완전하게 병행하고 있는 것부터, 고베·아카시 시내에서는 산요 전철선보다 빠르고, 단거리에서는 운임도 싼(장거리에서는 산요 전철이 싸다)JR선에 승객이 빼앗기고 있다.  또,JR선과 멀어지는 아카시 - 히메지간의 이용자도 양 회사의 역이 접하는 아카시나 다루미 등에서 JR선으로 많이 이동하는 현상을 볼 수 있다. 이 때문에, 「1day 티켓」등의 특별 기획 승차권을 발매해 승객의 이탈 방지나 신규 획득을 목표로 하고 있다. 또, 최근에는 텔레비전 광고(간사이의 방송국 한정)도 내보내 여객 획득에 노력하고 있다.\n",
      "\n",
      "전역의 자동 개찰화, 아보시 선의 1인 승무화, 주요역 이외의 각 역의 순회역화(실질적인 무인화로 관계자의 정기적 순회와 모니터 카메라 원격 감시를 실시한다), 보통 열차의 일부의 편성 단축 등 철저한 합리화를 실시하고, 한편으로 한신·아와지 대지진 후에 기획된 한신 우메다로의 직통 특급 운행을 실현해 일정한 성공을 거두고 있다. 그러나, 산요 전철선 연선에서 고베 시 중심부에 가기 위해서는 반드시 고베 고속 철도의 운임이, 그리고 오사카 방면으로 가기 위해서는 한신 전기 철도 또는 한큐 전철의 운임이 가산되기 때문에, 이것이 JR서일본과의 경쟁의 족쇄가 되어 있다라는 지적도 많다.\n",
      "\n",
      "고베 고속 철도의 개업 이후, 한큐 전철과 한신 전기 철도가 각각 산요 전철의 발행필주식 총수의 5%를 보유하고 있었지만 1998년에 한큐가 산요 구간으로의 노선 연장을 휴지한 후에 한큐가 한신에 보유주식을 매각했기 때문에, 현재는 한신 전기 철도가 필두 주주로 되어있다. 다만,2006년 10월 1일부로 동사는 한큐 전철과 함께 한큐 한신 홀딩스의 100%출자 자회사가 되어, 경영 통합하고 있다.\n",
      "\n",
      "Content: 본적은 충청북도 영동군이고, 철도공무원인 아버지를 따라 경북 구미에서 국민학교를 다닌 후, 1958년 군산중학교, 1961년 전주고등학교, 1965년 서울대학교 문리과대학 정치학과를 각각 졸업하였으며 대학 재학 중 한일회담 반대운동(학생운동 6.3세대)에 참여하였다 . 1966년 제6회 사법시험에 합격한 후 서울대학교 사법대학원을 거쳐 1968년 육군 법무관으로 병역을 마쳤다. \n",
      "\n",
      "1971년 전역 후 검사로 부산지방검찰청, 서울지방검찰청 인천지청, 서울지방검찰청, 대전지방검찰청 등에서 근무하였다. 대전지검 근무 시절인 1980년 8월 상부의 파견 명령으로 국가보위입법회의 내무위원으로 차출되어  교통사고처리특례법, 연좌제 폐지 등에 관여 했으며, 1981~1982년 인사이동으로 대검찰청 형사과장으로 근무하였고, 1983년 대검찰청 공안과장으로 발령받아 검찰내 공안 분야 보직을 처음 담당하게 되었으며, 1985년 미문화원방화사건 법정소요 사태 등을 계기로 벌어진 인사이동으로 서울지방검찰청 공안부장으로 보임되었다. \n",
      "\n",
      "서울지방검찰청 공안부장검사로 재직하였을 때 박종철 고문사건의 진상을 밝히는 중요한 역할을 했다 . 1987년 1월 14일 저녁 최환 부장검사가 홀로 근무하고 있는 부장검사실에 대공 경찰관 2명이 찾아와 경찰 조사 중이던 대학생이 변사했다는 발생 보고를 하면서 사망원인은 심장마비 급사이고 유족도 화장에 동의했으니 시신을 바로 인도하여 화장할 수 있게 지휘해 달라고 요청했다. 그러나 경찰의 '고문치사' 가능성을 직감한 최 부장검사는 \"두 분도 학교다니는 아이들이 있겠지요. 세상의 어느 부모가 서울로 유학보낸 자녀가 사망했다는데 그 자녀의 얼굴조차 보지 않고 바로 화장하겠다고 나서겠습니까? 화장이든 매장이든 그 전에 부모가 마지막으로 얼굴이라도 보아야 하지 않겠습니까? 처지를 바꾸어 생각해 보세요.\"라고 반박하고 이어 경찰이 원하는 화장을 허락하라고 빗발치는 청와대, 안기부, 검찰 내부 등 여러 압력 전화들에 굴하지 않고 경찰의 요청을 끝내 거부하였다. 그리고 경찰에 '사체보존명령'을 내려 시신이 훼손되지 못하게 한 후, 용산경찰서장 명의로 정식으로 변사사건 발생 보고 및 지휘건의를 하도록 지휘하였다 . 그 다음 날 휘하에 배속된 형사부 소속 안상수 검사 를 부검 장소인 한양대학교 병원으로 보내어 사망원인을 규명하기 위한 부검을 지휘하였다. 최 부장검사의 부검 지휘가 있었음에도 불구하고 이를 거부하던 강민창 치안본부장과 전화로 1시간 이상 논쟁을 벌이면서 \"경찰이 치안본부장의 명령에 따라 집단적으로 부검을 거부하면서 계속 시신을 내어 놓지 않으면 공무집행방해죄의 현행범으로 치안본부장을 체포하겠다\"는 취지로 끈질기게 설득한 노력 덕분에 한양대학교병원에서 박종철 열사의 시신에 대한 부검이 황적준 박사의 집도에 따라 이루어질 수 있었다.\n",
      "\n",
      "이 부검으로 인하여 박종철 열사가 치안본부가 당초 언론에 밝혔던 것처럼 '탁 치니 억' 하고 심장마비로 죽은 것이 아니라 대공 수사당국의 물고문 도중 질식사한 것이라는 사실이 만천하에 드러나게 되었다 . 온갖 압력을 물리치고 박종철 열사의 시신을 확보하여 부검을 관철시킬 수 있었던 연유로 다른 의문사들과 달리 명백한 '고문치사' 사망원인이 규명됨으로써 역사의 분수령이 된 박종철 고문치사사건이 은폐되지 않도록 막았다는 평가를 받는다. 최 부장검사는 천신만고 끝에 부검을 관철하여 사망원인을 규명했으나, 이후 고문치사 경찰관들에 대한 검찰 수사 단계에 이르러 철저히 수사하겠다는 최 부장검사의 의지와 달리 상부의 지시에 의해 최 부장검사가 제외되고 다른 부서를 중심으로 검찰수사팀이 구성됨으로써 정착 고문경찰관들에 대한 수사단계에서는 최 부장검사가 배제되었음 이 2차례에 걸친 정부의 과거사 조사 결과 밝혀지기도 했다 .\n",
      "\n",
      "1988년 서울지검 남부지청 차장검사를 거쳐 1989년 대구지검 차장검사를 역임했는데, 이 시기에 수질오염 수사의 효시인 대구 '낙동강 페놀 유출 사건'이 발생되자 검찰 수사를 통해 하루 만에 두산전자의 페놀 유출 을 밝혀내 처벌했다.\n",
      "\n",
      "1992~1993년 서울지방검찰청 남부지청 지청장으로 근무하면서 조직폭력배(일명 '용팔이')를 동원해 1987년 통일민주당 창당을 방해했던 배후가 장세동 전 안기부장임을 밝혀내고 장 전 안기부장을 구속기소 하였다 .\n",
      "\n",
      "김영삼 대통령이 집권한 이후 1993년 검사장(대검찰청 공안부장)이 되었고, 1994~1995년 법무부 검찰국장으로서 광복50주년 기념 '일반사면(대사면)' 실무를 총괄했다.\n",
      "\n",
      "1995년 9월 17일 서울지방검찰청 검사장에 보임되어 1997년 1월까지 재직하는 동안 5.18.민주화운동 및 12.12.사태의 주범인 전두환, 노태우 전 대통령들을 수사하는 특별수사본부를 서울지검내에 설치한 후, 두 전직 대통령들을 구속기소 하고 관련자들을 엄단 함으로써 헌정중단과 부정비리에 대한 형사처벌을 진두지휘 하였다.\n",
      "\n",
      "1999년 부산고등검찰청 검사장을 끝으로 공직에서 물러난 후 6.25.전쟁 당시 양민피해 사례인 '노근리 사건’의 정부조사단 자문위원으로 활동하면서 미국 클린턴 미국 대통령의 공식사과를 이끌어내는데 크게 기여 하였다. 이후 2000년 제16대 국회의원 선거에서 자유민주연합 후보로 대전광역시 대덕구 선거구에 출마하였으나 낙선 하였다. \n",
      "\n",
      "공직 퇴임 후에는 종래 법조관행과 달리 로펌에 들어가지 않았고, 이른 바 '전관예우'에 따른 영리를 추구하지 않았음이 인정되어 2013년 서울지방변호사회장이 주최한 '전관예우 타파를 위한 서울지방변호사회 정책세미나'에서 좌장을 맡았고, 2013년 5월 변호사회 회보에 퇴임 고위공직자의 모범사례로 \"전관예우는 스스로의 발등을 찍는 일 - 나눔과 상생을 실천해야\" 제목의 특별취재 대상 이 된 바 있다. 또한 사단법인 한국언론사협회가 주최하고 시정일보, 뉴미디어코리아 등 협회 회원사가 공동 주관하는 2013년 국제평화언론대상에서 \"국민의 권익보호를 위해 국민의 법률적 보호를 통한 사회적 역할을 충실히 이행했다\"며 대민봉사에 기여한 공로를 인정하여 대민봉사대상을 수상했다. \n",
      "\n",
      "한편, 전, 노 두 전직 대통령들을 구속기소했던 5.18. 수사책임자로서 5.18.의 역사적 진실을 왜곡 하는 북한침투설 등이 허구 임을 증언 하고, 5.18.이 지니는 인권과 민주주의의 가치, 화해와 상생의 협력을 강조 함으로써 2017년 광주시로부터 '명예광주시민' 으로 선정 되었고, 5.18 진상규명을 위한 조사위원회 상임위원 후보로 거론 되기도 하였다.\n",
      "\n",
      "Content: 2008년 원정화가 국가보안법 위반 혐의로 구속 기소되었다. 2001년에 위장 탈북한 원정화는 미인계로 육군 장교의 애인이 되어 기밀을 빼내었다. 징역 5년을 선고받고 2013년 만기 출소했다. 조선민주주의인민공화국은 미인계 전술을 매우 적극적으로 구사하는 나라로 알려져 있다. 타겟인 남한의 요직에 있는 공무원, 정치인들이 대부분 남자들이기 때문이다.  2017년 김정남 암살 사건 당시, 원정화는 그정도 타겟에 대한 미인계 암살이면 선불로만 100만 달러 정도를 받을 수 있다고 증언했다. \n",
      "\n",
      "2010년 4월 러시아 야당 및 반체제 인사 6명이 모델로 추정되는 여성과 성관계하는 동영상을 한 시민단체가 인터넷에 공개해 망신을 당하기도 했다. 푸틴 대통령을 반대하는 글을 써온 자유주의 성향의 풍자 작가인 빅토르 셴데로비치, 활동이 금지된 전국볼셰비키당의 창립자 에두아르드 리모노프, 반체제 운동가인 알렉산드르 벨로프 등이 예카테리나 게라시모바(일명 카챠)라는 여성과 각기 다른 날 같은 침실에서 성관계를 갖는 장면이 폭로되었다. 카챠는 FSB 요원일 가능성이 높다.\n",
      "\n",
      "2011년 12월부터 2013년 6월까지 5회에 걸쳐, 서울 강남구 삼성동 소재 삼성그룹 이건희 회장 자택과 김인 삼성 SDS고문이 전세계약한 논현동 집에서 이건희 회장이 다수의 고급창녀들과 성매매를 하고, 500만원 돈봉투를 각각 지급하는 동영상이 방송에 폭로되었다. CJ그룹 선모 부장이 동생과 함께 창녀들을 사주해 촬영한 혐의로 구속기소되었으나, 무죄를 주장중이다. CJ제일제당 전 부장 선모 씨(55)가 이재현 CJ 회장(57)의 측근으로서, 이 회장의 ‘금고지기’로 불리는 S씨와 e메일로 동영상 문제를 상의한 정황을 검찰이 확인했다.  선모 부장은 고 이맹희 씨제이그룹 명예회장의 의전을 담당했던 것으로 확인됐다. 이 때문에 해당 동영상이 고 이맹희 회장과 이건희 회장 간 상속권 분쟁 과정에서 추진된 것이 아니냐는 의혹이 제기되었다.  동영상을 촬여한 이들은 이건희 회장에게 폭로를 2회 공갈하여 6억원, 3억원을 받아냈다.\n",
      "\n",
      "2016년 4월 2일 블라디미르 푸틴 러시아 대통령의 정적 중 한 명으로 꼽혀왔던 미하일 카시야노프 인민자유당 당수가 개인비서인 나탈리아 펠레바인과 성관계를 갖는 동영상이 방송을 통해 적나라하게 공개됐다. 두 사람이 나눈 민망한 성적 대화들뿐만 아니라 분열된 야권의 다른 인사들에 대한 독설이 방송을 타고 나왔다. \n",
      "\n",
      "2016년 10월 19일, 고영태가 범삼성가이지만 삼성그룹과 대립중인 JTBC 방송사에 최초로 박근혜-최순실 게이트를 폭로했다. 고영태의 폭로로 박근혜가 탄핵, 구속되었다. 삼성그룹 이재용 부회장도 구속되었으며, 홍석현 JTBC 회장이 삼성그룹 경영권을 빼앗을 것이란 보도도 있었다. 한편, 고영태 대해 '남창' 이라며 콤프로마트 공작을 벌인 탄핵반대 모임과 일부 단체들의 배후에 대해서는 알려진 바는 없다.\n",
      "\n",
      "도널드 트럼프 대통령이 러시아 FSB의 콤프로마트 미인계 공작에 걸려 꼭두각시가 된 것은 아닌가 하는 의혹이 있다. 미스 유니버스 조직위원회를 소유하고 있는 도널드 트럼프가 2013년 러시아 모스크바에서 열린 미스 유니버스 대회를 위해 5성급 호텔인 모스크바 리츠 칼튼 호텔에 묵었다. 여기서 트럼프는 많은 미녀들과 섹스 파티를 했고, 이를 FSB가 동영상을 촬영해, 이 동영상으로 협박을 하며 트럼프를 뒤에서 조종하고 있다는 의혹이다. \n",
      "\n",
      "2013년 중국계 미국인으로서는 처음으로 중국 베이징 주재 미국 대사를 맡았다가 2013년 돌연 사의를 표했던 게리 로크(63)가 2015년 이혼했다. 베이징의 여성 사업가와 불륜을 저질렀고, 이 여성이 중국 정보부 요원이며, 콤프로마트 미인계 공작에 걸린 것으로 보도되었다. \n",
      "\n",
      "2016년 10월 네덜란드 정부가 론 켈러 베이징 주재 대사를 긴급 소환해 직무를 정지시켰다. 켈러 대사는 대사관 직원으로 채용된 중국 여성과 비밀리에 연인 관계를 맺어왔다. 중국은 과거부터 정보를 수집하기 위해 여성 간첩을 활용하는 미인계(honey trap)에 능하다는 말을 들어왔다.  요즘의 미인계는 반드시 동영상을 촬영하며, 동영상을 폭로한다고 협박하면, 대부분은 꼭두각시가 된다.\n",
      "\n",
      "Content: 2020년 1월을 기준으로 크로스링스는 전체 선로의 절반을 설치하였다. \n",
      "\n",
      "2020년 2월, 메트로링스는 이 노선이 기존 목표인 2021년 9월 대신 2022년에 개통할 것으로 보인다고 밝혔다.  메트로링스는 건설사인 크로스링스에 2억 3700만 달러의 합의금을 건네주었음에도 불구하고  메트로링스는 공사 지연의 원인으로 크로스링스가 공사를 9개월 늦게 시작하였고 설계 일부를 마무리하는 데 늦어졌고, 그와 동시에 에글린턴역에 영 지하철이 지어진 1950년대에 콘크리트 안에 매립된 배관이 발견되어서 굴착이 지연되었다고 밝혔다.  메트로링스의 비공개 문건에 따르면 경전철 공사 예산이 3억 3천만 달러 초과되었을 것으로 보인다\n",
      "\n",
      "2020년 3월 9일, 주정부는 경전철 공사로 침체된 상권을 도와주기 위해 에글린턴 애비뉴의 자영업자들에게 300만 달러를 지원할 것이라고 밝혔다. 교통부와 메트로링스는 또한 경전철의 부분 개통도 검토하였다. \n",
      "\n",
      "한편 크로스링스는 3월에 파머시역에 처음으로 경전철 승객 대기 공간을 설치하였다고 밝혔다. 이 시설은 사전에 만들어졌고 경전철 승강장 위에 설치해 고정하였다.  또한 크로스링스는 3월에 듀플렉스 애비뉴에 있던 굴착기 데니스와 레아가 임무를 마치고 지상으로 나왔다 2020년 4월 15일, 경전철 서부 터널 굴착기인 돈과 험버 또한 지상으로 나왔다. \n",
      "\n",
      "2020년 5월 4일, 메트로링스는 5호선 터널에 경전철 차량의 첫 시험 운행을 에글린턴 차량기지에서 킬스데일역까지 처음으로 완료하였다. 이 시험 운행은 이간거리, 선로 작업, 가공전력선과 통신을 확인하기 위해 이루어졌으며 터널을 최대 시속 25km로 오갔다.\n",
      "\n",
      "Content: 1939년 경주시 안강읍 신대리의 청안 이씨 충간공파의 시조 이한번(李漢蕃) 자손으로 태어나  안강제일초등학교, 경주중학교, 경주고등학교(7회)와 서울대학교 법학과를 졸업하고 제13회 고등고시 사법과에 합격하여 판사에 임용된 이정락은 서울지방법원 남부지원장과 서울가정법원. 서울형사지방법원 등에서 법원장을 역임하였다. 1993년 서울형사지방법원에서의 법원장을 마지막으로 30여년 간의 법관 생활을 마치고 변호사 개업을 하여 대한변호사협회 산하 법률구조재단의 초대 이사장을 맡아 법률 구조에 있어 기틀을 닦았다.  서울형사지방법원 법원장으로 있던 1992년에 법원에서 컴퓨터 속기 설명회를 가져 매우 좋은 평가를 받아 법원에 컴퓨터 속기가 도입되었다.  이정락은 변호사 사무소를 개업하면서  공평과 정의로 사회 고통 치유하는 힐링 변호사를 내걸고 있다. \n",
      "\n",
      "서울고등법원에서 재직할 때 이정락은 전두환 정부의 대표적인 공안 사건이었던 아람회 사건의 항소심 재판장을 맡아 \"'아람회의 목적이 구체적으로 특정되었다'고 볼 수 없을 뿐만 아니라 이 모임이 정부를 전복하겠다는 목적을 갖고 있었다고 단정할 증거가 없다\"는 이유로 국가보안법상 반국가단체 구성과 반국가단체 구성원과의 회합·통신·허위사실유포에 대해 무죄를 인정하면서 최고 징역 10년이었던 형량을 징역6년에서 집행유예 까지 낮춰 선고했으나 무죄 취지 판단에 대해 대법원에서 파기되었다. 또 진도 가족 간첩단 사건으로 1심에서 사형이 선고된 피고인에 대해 무기징역을 선고하면서 혐의를 모두 인정했다. \n",
      "\n",
      "노태우 비자금 사건에서 김우중 대우그룹 회장 변호사를 하면서 \"금전의 제공시기가 연말연시나 선거철등이고 제공장소도 공개된 장소였다.또 자금 제공의 목적도 통치자금으로 쓰도록 성금으로 전달한 것이니 만큼 대가성이 없었다\"며 \"뇌물공여가 인정되더라도 대우그룹이 아닌 시대의 관행이 뇌물 공여죄로 처벌받아야 한다.고 말했다. \n",
      "\n",
      "이정락은 대통령 후보였던 이회창의 법률지원단장을 맡으면서 후보의 대표적인 사조직으로 꼽힌 부국팀 회장을 맡았으며  이로 인하여 대기업으로부터 거액을 불법 후원받은 등의 이유로 검찰 조사를 받았다.\n",
      "\n",
      "사단법인 과학사랑∙나라사랑 관계자로부터 \"허쉬바흐 박사가 한국의 시골 초등학교를 보고 싶어한다\"는 말을 전해 들은 이정락은 2002년 10월23일에 허쉬바흐 박사와 함께 자신이 총동창회장으로 있는 모교인 안강제일초등학교를 방문하여 \"기왕이면 우리 고장, 우리 모교를 방문하는 것이 영광스러울 것 같아 추천했다”고 말하면서 \"허쉬바흐 박사는 외국의 어느 손님보다 귀한 손님이다\"며 \"앞으로 모교에서 노벨상 수상자가 탄생한다면 허쉬바흐 박사의 방문이 더욱 값지게 될 것”이라고 했다. \n",
      "\n",
      "안강읍 안강찰토마토작목반원을 비롯해 현곡면 현곡토마토작목반 회원들과 그 가족들이 불량씨앗을 판매한 종묘상을 상대로 소송을 하다 서울지방법원에서 패소한 것을 서울고등법원에 항소하는 과정에서 이정락 변호사가 \"고향주민들의 어려움을 덜어주겠다\"며 자청해서 무료변론 끝에 고법에서 승소를 했다. \n",
      "\n",
      "보수성향 변호사의 모임으로 알려진 헌변 회원이자 대한변호사협회 법률구조재단 이사장을 맡고 있던 2007년에  \"법률구조사업이야 말로 이제 우리 변호사들의 책임이고 의무\"라며 \"변호사의 노블리스 오블리쥬는 바로 법률구조운동에 헌신적으로 참여하고 실천하는 길\"이라고 말했다. \n",
      "\n",
      "중도 성향 변호사 모임을 자처하며 2005년에 창립된 '시민과 함께하는 변호사들이 \"법원 내 이념 편향적인 판사들의 활동에 대해 우려해왔고, 이른바 '튀는 판결'을 감시해야 한다\"면서 창립 5주년 기념 세미나에 참석했다. \n",
      "\n",
      "경주고도보존회와 재향 경우회 회장을 맡은 이정락은 2012년 3월 29일에 모교인 경주고등학교 1,2학년을 대상으로 미국 시인 ‘프로스트’의 ‘눈 오는 저녁 숲가에 서서’의 시 한구절인 ‘miles to go before I sleep, 잠들기 전에 가야할 길이 있다’를 인용하여 학생들이 미래에 가야하는 길에 대한 자세와 마음가짐에 관하여 3C(confidence, Concentration, Consistency)으로 강연하였다. \n",
      "\n",
      "박근혜 정부의 문화예술계 지원배제 명단 '블랙리스트'를 작성·집행을 주도한 혐의를 받은 김기춘과 조윤선의 변호인단으로 선임된 이정락은 항소심 결심 공판에서  \"피고인들이 순차적으로 공모해 청와대 정무수석실 주도로 태스크포스(TF)를 구성했다\"는 특검 주장에 대해 \"어떤 명목이든지 대통령 비서실장으로 실패한 대통령과 함께 처벌해야 한다는 억지 논리는 지금의 법치주의 하에서 도저히 허용될 수 없다\"면서 \"특검이 제시한 증거는 어느 공소사실에 부합하는지 밝히지 못하고 순차 공모부분 역시 추상적이고 애매하다\"고 반박하며 피고인들에 대해 관대한 처벌을 요청했다.\n",
      "\n",
      "Content: 브라이언 엡스타인이 1967년 8월 27일 사망한 이후, 비틀즈는 창작열에 불타올랐다. 그러나 그 전 밴드는 며칠 간을 충격 속에 보냈고, 폴 매카트니는 그들의 비탄을 통제할 한 가지 묘수를 떠올린다. 바로 스튜디오로 돌아가는 것이었다. 9월 5일부터 시작된 작업에서, 존 레논은 자신이 졸업한 고등학교의 학생이 비틀즈의 가사에서 숨겨진 의미를 찾는 공부를 하고 있다는 편지에서 영감을 받은 새 일렉트릭 곡을 가져왔다. 레논은 〈I Am the Walrus〉의 솔로 어쿠스틱 버전을 연주했고, 당시 상황을 엔지니어 제프 에머릭은 이렇게 회상했다. \"모두가 어리둥절 해 있었다. 멜로디는 크게 두 개로 구성돼 있었는데, 가사는 꽤나 넌센스해 보였다.\"  등장하는 해마는 루이스 캐럴의 시 〈해마와 목수〉 (《거울 나라의 앨리스》에서 등장)에서 가져왔다. 레논은 시에서 등장하는 해마가 실은 악당이었다는 사실을 뒤늦게 알고는 경악했다.p=185\n",
      "\n",
      "조지 마틴은 \"날 제외하고 너희끼리 뭘 하려는 심산이지?\"라고 말했다. 결국에는 모두가 트랙에 참여했다. 레논은 단순한 일렉트릭 형태의 반주를 붙였고, 매카트니는 스타가 박자를 지킬 수 있도록 탬버린을 흔들었다. 매카트니의 이러한 근면성실함은 밴드를 집중할 수 있도록 도왔고, 에머릭은 이 순간을 회상하며 \"폴의 가장 멋진 순간\"이라고 했다. 트랙은 후기 제작에서 강렬하며, 멍한 느낌을 가질 수 있었다. 초기에 받은 충격에도 아랑곳 않고, 마틴은 능수능란한 솜씨로 오케스트라 편곡을 작곡해 마치 어지럼증과 같은 느낌을 부여했다. 마치 달에서 온 사운드를 원했던 레논은, 자신의 목소리를 최대한 찌그려뜨려 달라는 부탁을 했다\n",
      "\n",
      "레논은 \"낱말들은 사실 별 의미가 없다\"며 \"사람들이 너무나 많은 추측을 했고, 그건 하나같이 말도 안 되는 것이었다. 'I am the Eggman?'의 진정한 뜻이 뭐냐고? 나도 잘 모른다. 그건 푸딩 그릇이 될 수도 있었다.\"고 말했다. 가사는 그 내부에 수많은 말장난이 포함돼 있다. \"Semolina pilchard\"는 런던의 마약계 경찰인 노먼 필처에 대한 언급이다. 그는 믹 재거나 키스 리처즈와 같은 록 스타를 잡아들이곤 했다. 그리고 \"The Eggman\"은 캐롤의 험티 덤티와 레논이 에릭 버든으로부터 들은, 그 무렵 한 소녀가 애니멀스의 프론트맨과 섹스 도중 그 위에서 달걀을 깨뜨렸다는 얘기에 기원이 있다. 이듬해의 화이트 앨범에서, 레논은 〈Glass Onion〉에 \"해마는 폴이다(The walrus was Paul)\"라는 곡의 언급을 했다. 이는 엡스타인의 죽음 이후 그룹을 규합할 수 있도록 도운 매카트니의 대한 그만의 감사 방법이었다\n",
      "\n",
      "[BM25 Retriever]\n",
      "Content: 이철희 장영자 어음 사기 사건은 1982년 5월 4일, 사채시장의 '큰손'으로 불리던 장영자와 남편 이철희가 어음사기 혐의로 검찰에 구속되면서 일어난 대규모 어음 사기사건이다. \\n\\n장영자는 국회의원과 국가안전기획부 차장을 지낸 이철희를 내세워 미모와 화려한 언변으로 고위층과 긴밀한 관계를 과시한 후 자금난을 겪고 있는 기업들에 자금지원 대가로 2배에서 최고 9배짜리 어음을 받아 이를 사채시장에 유통시키고 돈을 착복했다. 어음과 담보조로 받은 견질어음을 몽땅 시중에서 할인한 후 다시 굴리는 수법으로 6400억 원의 어음을 시중에 유통시켜 1400여 억 원을 사기로 취했다. \\n\\n이 사건으로 인해 소위 ‘장영자 후폭풍’이라는 정계·경제계는 물론 사회 각 분야에 엄청난 파문이 몰아쳤는데, 특히 경제계에 엄청난 파문이 일었다. 장영자는 ‘경제는 유통이다’라는 유명한 말로 항변했지만 어음이 한 바퀴 돌았을 때 어음을 발행한 기업들이 부도를 내고 무너지기 시작했다. 이 사건으로 한국에서 처음으로 금융실명제 논의가 본격화되기 시작하였다. 검찰 수사 결과 이철희·장영자 뒤에는 장영자의 형부이자 전두환의 처삼촌인 이규광이 버티고 있었고, 이 사건으로 두 사람은 물론 은행장 2명과 공영토건, 일신제강 등 내로라하는 기업인 등 모두 32명이 구속됐다. \\n\\n장영자·이철희는 모두 15년의 징역형을 선고받고 복역 중 이철희가 먼저 가석방된 뒤, 장영자는 복역 10년 만인 1992년 3월 역시 가석방으로 풀려났다. \\n\\n그러나 장영자는 1994년 다시 100억 원대의 어음 사기사건으로 구속되어 복역하였고, 2001년 5월에도 220억 원대의 구권화폐 사기 행각을 벌인 혐의로 구속됨으로써 세 번째로 복역하게 되었다.\n",
      "\n",
      "Content: 4대강 정비 사업\n",
      "2010년 8월 17일, 당일 방송예정이던 \"4대강 수심 6m ··· 누가 밀어 붙였나?\"에 국토해양부 4대강살리기 추진본부는 MBC PD 수첩이 방송을 앞두고 사전배포한 보도자료가 명백한 허위사실인데도 신문과 방송, 인터넷 등을 통해 급속하게 확산되고 있어 방송금지 가처분 신청을 서울남부지방법원에 냈다고 밝혔다.  그러나 서울남부지법 민사51부(양재영 부장판사)는 국토해양부가 낸 방송금지 가처분 신청을 기각했다. 재판부는 결정문에서, \"기록만으로는 방송예정인 프로그램의 내용이 명백히 진실이 아니고 방송 목적이 공공의 이익에 부합하지 않는다고 보기 어렵다. 또한 방송이 이뤄진다고 해서 신청인에게 중대하고 회복하기 어려운 손해를 입힐 우려가 있다는 점에 대한 소명이 부족하다\"라고 밝혔다.  'PD수첩' 제작진에 주장에 의하면 김재철 문화방송 사장은 임원회의에서 사규위반을 이유로 처음에 방송보류를 지시했다고 한다.  그러나 이 방송은 2010년 8월 24일에 방송된다.\n",
      "\n",
      "Content: 4대강 정비 사업\n",
      "2010년 8월 17일, 당일 방송예정이던 \"4대강 수심 6m ··· 누가 밀어 붙였나?\"에 국토해양부 4대강살리기 추진본부는 MBC PD 수첩이 방송을 앞두고 사전배포한 보도자료가 명백한 허위사실인데도 신문과 방송, 인터넷 등을 통해 급속하게 확산되고 있어 방송금지 가처분 신청을 서울남부지방법원에 냈다고 밝혔다.  그러나 서울남부지법 민사51부(양재영 부장판사)는 국토해양부가 낸 방송금지 가처분 신청을 기각했다. 재판부는 결정문에서, \"기록만으로는 방송예정인 프로그램의 내용이 명백히 진실이 아니고 방송 목적이 공공의 이익에 부합하지 않는다고 보기 어렵다. 또한 방송이 이뤄진다고 해서 신청인에게 중대하고 회복하기 어려운 손해를 입힐 우려가 있다는 점에 대한 소명이 부족하다\"라고 밝혔다.  'PD수첩' 제작진에 주장에 의하면 김재철 문화방송 사장은 임원회의에서 사규위반을 이유로 처음에 방송보류를 지시했다고 한다.  그러나 이 방송은 2010년 8월 24일에 방송된다.\n",
      "\n",
      "Content: 1959년 태어난 조영철은 경북고등학교와 서울대학교 법학과를 졸업하고 제25회 사법시험 합격했다. 제15기 사법연수원과 군 법무관을 마치고 1989년 대구지방법원 판사에 임용되었다. 이후 대구지방법원 김천지원, 수원지방법원, 서울지방법원, 서울고등법원으로 전보되어 판사를 하다가 대법원 재판연구관을 거쳐 부장판사에 승진하여 대구지방법원, 서울중앙지방법원, 수원지방법원, 광주고등법원, 서울고등법원에서 부장판사로 재판장을 하였으며 2014년에는 서울중앙지방법원 민사수석부장판사를 하다가 2015년 2월 법원장으로 승진하여 의정부지방법원 법원장에 취임하였다. 의정부지방법원에서 법원장으로 2년동안 재직하다가 서울고등법원 부장판사로 복귀하여 재판을 하였다.\\n\\n공사계약에서 리베이트 약정은 무효, 군부대에서 벌목작업에 참여했다 사고로 숨진 병사를 월북자로 알리고 가족을 수십 년간 감시한 사안에 대해 국가의 손해배상책임을 인정, 대학교 동창회장이 일정 금액을 발전기금으로 내도록 한 동창회 선거규정 무효라는 판결을 했었던 조영철은 박근혜 정부에서 국정농단 혐의를 받은 최순실 측에서 \"이화여자대학교 학사비리 재판을 하면서 1심과 같이 징역3년 등을 선고한 바가 있는 재판장 조영철이 재판을 불공정하게 할 우려가 있어 이에 대한 기피를 신청한다\"며 재판부 기피 신청을 했다. \\n\\n오패산 터널 인근에서 사제 총기를 난사해 경찰관을 숨지게 한 혐의로 1심에서 무기징역을 선고받은 성병대가 법정에서 재판장인 조영철에게 \"이번 법관 인사로 좌·우배석 판사님들은 다 바뀌었는데 왜 재판장님만 안 바뀌는 건가요. 재판장님이 경찰의 청탁을 받았다는 제 이야기가 맞아떨어지는 것 아닙니까?\"고 항의하면서 자신의 사건을 \"다른 재판부로 보내달라\"고 했다. \\n\\n법원장으로 있다가 고등법원 부장판사로 재판부를 근무를 했던 조영철은 2019년 1월 28일에 있었던 법원인사에서 대구고등법원 법원장에 임명되었다.\n",
      "\n",
      "Content: 구·효고 전기 궤도에 유래하는 효고 - 아카시간의 궤도와 구·고베 히메지 전기 철도에 유래하는 아카시 역앞(현재의 산요아카시) - 히메지 역앞(현재의 산요히메지) 사이의 철도가 노선의 모체가 되고 있다.\n",
      "\n",
      "선행해 개통한 효고 전기 궤도(이하 '효철')는 개업 이래 경영이 불안정한 면이 있어, 서아카시로의 연장 시에 별도의 회사인 고베 히메지 전기 철도(이하 '고베 철도')를 설립하는 것으로 리스크 회피를 도모했다. 그러나 반슈 철도(현재의 JR가코가와 선의 전신)의 경영진에 의한 효철의 적대적 매수에 의해 반슈 철도 산하가 된 효철과 구 효철파의 고베 철도는 소원하게 되었던 시기도 있었다. 그러한 혼란 수습과 아울러 자사의 매전처 확보를 노린 전쟁 전의 대기업 전력회사·우지강전기(간사이 전력의 전신의 하나)가 양 회사를 병합 해 자사의 철도 사업 부문으로 했다. 후에 우지강전기가 본업에의 추출에 의해 철도 부문을 분리했을 때에 새롭게 설립된 것이 현재의 산요 전기 철도이다.\n",
      "\n",
      "합병전 2회사의 노선 규격의 상위 등 곤란한 조건을 극복해 직통 운전을 실현해,현청 소재지의 고베와 하리마의 중심지인 히메지를 직결하는 도시간 연락 노선으로 성장했다. 그리고 1940년에는 연선에서의 공원 수송을 목적으로 지선의 아보시 선을 개통시켰다.\n",
      "\n",
      "전후 운수성의 63형 전철 도입을 기회로 고속 철도로의 탈피를 도모한다. 또 가와사키 차량→가와사키 중공업과 협력해,850형 로망스카의 대형차량의 채용이나 2000계등의 선진적인 설계의 전철의 투입 등 적극성을 발휘, 특히 1962년에는 고속 전동차로서 일본 최초의 올 알루미늄차를 도입하고 있다. 또 전후도 오랜 세월에 걸쳐서 고베시안에 있어서의 병용 궤도 구간(노면 주행 구간)이 잔존해 운행의 어려움이 되고 있었지만,1968년에는 고베 고속 철도의 지하선 직통을 개시해 병용 궤도를 폐지하는 것과 동시에 게이한신 급행 전철(현·한큐 전철) 및 한신 전기 철도로부터의 직통열차가 노선 연장하게 되어 산노미야·오사카 방면과의 관계를 강하게 할 수 있었다.\n",
      "\n",
      "1970년대에 들어가면, 연선의 하리마 임해 공업 지대에서 제철업의 침체 경향이 표면화했다. 아보시 선의 건설 이유였던 신일본제철(←후지 제철←일본제철)히로하타 제철소의 용광로 휴지와 거기에 따르는 대규모 정리해고 등은 산요 전기 철도의 경영에도 큰 타격을 주었다. 연선에는 교외 지역이 많아서 모터라제이션의 영향도 심각하게 다가왔다. 게다가 고베 시내 및 아카시 역 부근까지 서일본 여객철도(JR서일본)의 산요 본선(JR 고베 선)과 완전하게 병행하고 있는 것부터, 고베·아카시 시내에서는 산요 전철선보다 빠르고, 단거리에서는 운임도 싼(장거리에서는 산요 전철이 싸다)JR선에 승객이 빼앗기고 있다.  또,JR선과 멀어지는 아카시 - 히메지간의 이용자도 양 회사의 역이 접하는 아카시나 다루미 등에서 JR선으로 많이 이동하는 현상을 볼 수 있다. 이 때문에, 「1day 티켓」등의 특별 기획 승차권을 발매해 승객의 이탈 방지나 신규 획득을 목표로 하고 있다. 또, 최근에는 텔레비전 광고(간사이의 방송국 한정)도 내보내 여객 획득에 노력하고 있다.\n",
      "\n",
      "전역의 자동 개찰화, 아보시 선의 1인 승무화, 주요역 이외의 각 역의 순회역화(실질적인 무인화로 관계자의 정기적 순회와 모니터 카메라 원격 감시를 실시한다), 보통 열차의 일부의 편성 단축 등 철저한 합리화를 실시하고, 한편으로 한신·아와지 대지진 후에 기획된 한신 우메다로의 직통 특급 운행을 실현해 일정한 성공을 거두고 있다. 그러나, 산요 전철선 연선에서 고베 시 중심부에 가기 위해서는 반드시 고베 고속 철도의 운임이, 그리고 오사카 방면으로 가기 위해서는 한신 전기 철도 또는 한큐 전철의 운임이 가산되기 때문에, 이것이 JR서일본과의 경쟁의 족쇄가 되어 있다라는 지적도 많다.\n",
      "\n",
      "고베 고속 철도의 개업 이후, 한큐 전철과 한신 전기 철도가 각각 산요 전철의 발행필주식 총수의 5%를 보유하고 있었지만 1998년에 한큐가 산요 구간으로의 노선 연장을 휴지한 후에 한큐가 한신에 보유주식을 매각했기 때문에, 현재는 한신 전기 철도가 필두 주주로 되어있다. 다만,2006년 10월 1일부로 동사는 한큐 전철과 함께 한큐 한신 홀딩스의 100%출자 자회사가 되어, 경영 통합하고 있다.\n",
      "\n",
      "Content: 본적은 충청북도 영동군이고, 철도공무원인 아버지를 따라 경북 구미에서 국민학교를 다닌 후, 1958년 군산중학교, 1961년 전주고등학교, 1965년 서울대학교 문리과대학 정치학과를 각각 졸업하였으며 대학 재학 중 한일회담 반대운동(학생운동 6.3세대)에 참여하였다 . 1966년 제6회 사법시험에 합격한 후 서울대학교 사법대학원을 거쳐 1968년 육군 법무관으로 병역을 마쳤다. \n",
      "\n",
      "1971년 전역 후 검사로 부산지방검찰청, 서울지방검찰청 인천지청, 서울지방검찰청, 대전지방검찰청 등에서 근무하였다. 대전지검 근무 시절인 1980년 8월 상부의 파견 명령으로 국가보위입법회의 내무위원으로 차출되어  교통사고처리특례법, 연좌제 폐지 등에 관여 했으며, 1981~1982년 인사이동으로 대검찰청 형사과장으로 근무하였고, 1983년 대검찰청 공안과장으로 발령받아 검찰내 공안 분야 보직을 처음 담당하게 되었으며, 1985년 미문화원방화사건 법정소요 사태 등을 계기로 벌어진 인사이동으로 서울지방검찰청 공안부장으로 보임되었다. \n",
      "\n",
      "서울지방검찰청 공안부장검사로 재직하였을 때 박종철 고문사건의 진상을 밝히는 중요한 역할을 했다 . 1987년 1월 14일 저녁 최환 부장검사가 홀로 근무하고 있는 부장검사실에 대공 경찰관 2명이 찾아와 경찰 조사 중이던 대학생이 변사했다는 발생 보고를 하면서 사망원인은 심장마비 급사이고 유족도 화장에 동의했으니 시신을 바로 인도하여 화장할 수 있게 지휘해 달라고 요청했다. 그러나 경찰의 '고문치사' 가능성을 직감한 최 부장검사는 \"두 분도 학교다니는 아이들이 있겠지요. 세상의 어느 부모가 서울로 유학보낸 자녀가 사망했다는데 그 자녀의 얼굴조차 보지 않고 바로 화장하겠다고 나서겠습니까? 화장이든 매장이든 그 전에 부모가 마지막으로 얼굴이라도 보아야 하지 않겠습니까? 처지를 바꾸어 생각해 보세요.\"라고 반박하고 이어 경찰이 원하는 화장을 허락하라고 빗발치는 청와대, 안기부, 검찰 내부 등 여러 압력 전화들에 굴하지 않고 경찰의 요청을 끝내 거부하였다. 그리고 경찰에 '사체보존명령'을 내려 시신이 훼손되지 못하게 한 후, 용산경찰서장 명의로 정식으로 변사사건 발생 보고 및 지휘건의를 하도록 지휘하였다 . 그 다음 날 휘하에 배속된 형사부 소속 안상수 검사 를 부검 장소인 한양대학교 병원으로 보내어 사망원인을 규명하기 위한 부검을 지휘하였다. 최 부장검사의 부검 지휘가 있었음에도 불구하고 이를 거부하던 강민창 치안본부장과 전화로 1시간 이상 논쟁을 벌이면서 \"경찰이 치안본부장의 명령에 따라 집단적으로 부검을 거부하면서 계속 시신을 내어 놓지 않으면 공무집행방해죄의 현행범으로 치안본부장을 체포하겠다\"는 취지로 끈질기게 설득한 노력 덕분에 한양대학교병원에서 박종철 열사의 시신에 대한 부검이 황적준 박사의 집도에 따라 이루어질 수 있었다.\n",
      "\n",
      "이 부검으로 인하여 박종철 열사가 치안본부가 당초 언론에 밝혔던 것처럼 '탁 치니 억' 하고 심장마비로 죽은 것이 아니라 대공 수사당국의 물고문 도중 질식사한 것이라는 사실이 만천하에 드러나게 되었다 . 온갖 압력을 물리치고 박종철 열사의 시신을 확보하여 부검을 관철시킬 수 있었던 연유로 다른 의문사들과 달리 명백한 '고문치사' 사망원인이 규명됨으로써 역사의 분수령이 된 박종철 고문치사사건이 은폐되지 않도록 막았다는 평가를 받는다. 최 부장검사는 천신만고 끝에 부검을 관철하여 사망원인을 규명했으나, 이후 고문치사 경찰관들에 대한 검찰 수사 단계에 이르러 철저히 수사하겠다는 최 부장검사의 의지와 달리 상부의 지시에 의해 최 부장검사가 제외되고 다른 부서를 중심으로 검찰수사팀이 구성됨으로써 정착 고문경찰관들에 대한 수사단계에서는 최 부장검사가 배제되었음 이 2차례에 걸친 정부의 과거사 조사 결과 밝혀지기도 했다 .\n",
      "\n",
      "1988년 서울지검 남부지청 차장검사를 거쳐 1989년 대구지검 차장검사를 역임했는데, 이 시기에 수질오염 수사의 효시인 대구 '낙동강 페놀 유출 사건'이 발생되자 검찰 수사를 통해 하루 만에 두산전자의 페놀 유출 을 밝혀내 처벌했다.\n",
      "\n",
      "1992~1993년 서울지방검찰청 남부지청 지청장으로 근무하면서 조직폭력배(일명 '용팔이')를 동원해 1987년 통일민주당 창당을 방해했던 배후가 장세동 전 안기부장임을 밝혀내고 장 전 안기부장을 구속기소 하였다 .\n",
      "\n",
      "김영삼 대통령이 집권한 이후 1993년 검사장(대검찰청 공안부장)이 되었고, 1994~1995년 법무부 검찰국장으로서 광복50주년 기념 '일반사면(대사면)' 실무를 총괄했다.\n",
      "\n",
      "1995년 9월 17일 서울지방검찰청 검사장에 보임되어 1997년 1월까지 재직하는 동안 5.18.민주화운동 및 12.12.사태의 주범인 전두환, 노태우 전 대통령들을 수사하는 특별수사본부를 서울지검내에 설치한 후, 두 전직 대통령들을 구속기소 하고 관련자들을 엄단 함으로써 헌정중단과 부정비리에 대한 형사처벌을 진두지휘 하였다.\n",
      "\n",
      "1999년 부산고등검찰청 검사장을 끝으로 공직에서 물러난 후 6.25.전쟁 당시 양민피해 사례인 '노근리 사건’의 정부조사단 자문위원으로 활동하면서 미국 클린턴 미국 대통령의 공식사과를 이끌어내는데 크게 기여 하였다. 이후 2000년 제16대 국회의원 선거에서 자유민주연합 후보로 대전광역시 대덕구 선거구에 출마하였으나 낙선 하였다. \n",
      "\n",
      "공직 퇴임 후에는 종래 법조관행과 달리 로펌에 들어가지 않았고, 이른 바 '전관예우'에 따른 영리를 추구하지 않았음이 인정되어 2013년 서울지방변호사회장이 주최한 '전관예우 타파를 위한 서울지방변호사회 정책세미나'에서 좌장을 맡았고, 2013년 5월 변호사회 회보에 퇴임 고위공직자의 모범사례로 \"전관예우는 스스로의 발등을 찍는 일 - 나눔과 상생을 실천해야\" 제목의 특별취재 대상 이 된 바 있다. 또한 사단법인 한국언론사협회가 주최하고 시정일보, 뉴미디어코리아 등 협회 회원사가 공동 주관하는 2013년 국제평화언론대상에서 \"국민의 권익보호를 위해 국민의 법률적 보호를 통한 사회적 역할을 충실히 이행했다\"며 대민봉사에 기여한 공로를 인정하여 대민봉사대상을 수상했다. \n",
      "\n",
      "한편, 전, 노 두 전직 대통령들을 구속기소했던 5.18. 수사책임자로서 5.18.의 역사적 진실을 왜곡 하는 북한침투설 등이 허구 임을 증언 하고, 5.18.이 지니는 인권과 민주주의의 가치, 화해와 상생의 협력을 강조 함으로써 2017년 광주시로부터 '명예광주시민' 으로 선정 되었고, 5.18 진상규명을 위한 조사위원회 상임위원 후보로 거론 되기도 하였다.\n",
      "\n",
      "Content: 2008년 원정화가 국가보안법 위반 혐의로 구속 기소되었다. 2001년에 위장 탈북한 원정화는 미인계로 육군 장교의 애인이 되어 기밀을 빼내었다. 징역 5년을 선고받고 2013년 만기 출소했다. 조선민주주의인민공화국은 미인계 전술을 매우 적극적으로 구사하는 나라로 알려져 있다. 타겟인 남한의 요직에 있는 공무원, 정치인들이 대부분 남자들이기 때문이다.  2017년 김정남 암살 사건 당시, 원정화는 그정도 타겟에 대한 미인계 암살이면 선불로만 100만 달러 정도를 받을 수 있다고 증언했다. \n",
      "\n",
      "2010년 4월 러시아 야당 및 반체제 인사 6명이 모델로 추정되는 여성과 성관계하는 동영상을 한 시민단체가 인터넷에 공개해 망신을 당하기도 했다. 푸틴 대통령을 반대하는 글을 써온 자유주의 성향의 풍자 작가인 빅토르 셴데로비치, 활동이 금지된 전국볼셰비키당의 창립자 에두아르드 리모노프, 반체제 운동가인 알렉산드르 벨로프 등이 예카테리나 게라시모바(일명 카챠)라는 여성과 각기 다른 날 같은 침실에서 성관계를 갖는 장면이 폭로되었다. 카챠는 FSB 요원일 가능성이 높다.\n",
      "\n",
      "2011년 12월부터 2013년 6월까지 5회에 걸쳐, 서울 강남구 삼성동 소재 삼성그룹 이건희 회장 자택과 김인 삼성 SDS고문이 전세계약한 논현동 집에서 이건희 회장이 다수의 고급창녀들과 성매매를 하고, 500만원 돈봉투를 각각 지급하는 동영상이 방송에 폭로되었다. CJ그룹 선모 부장이 동생과 함께 창녀들을 사주해 촬영한 혐의로 구속기소되었으나, 무죄를 주장중이다. CJ제일제당 전 부장 선모 씨(55)가 이재현 CJ 회장(57)의 측근으로서, 이 회장의 ‘금고지기’로 불리는 S씨와 e메일로 동영상 문제를 상의한 정황을 검찰이 확인했다.  선모 부장은 고 이맹희 씨제이그룹 명예회장의 의전을 담당했던 것으로 확인됐다. 이 때문에 해당 동영상이 고 이맹희 회장과 이건희 회장 간 상속권 분쟁 과정에서 추진된 것이 아니냐는 의혹이 제기되었다.  동영상을 촬여한 이들은 이건희 회장에게 폭로를 2회 공갈하여 6억원, 3억원을 받아냈다.\n",
      "\n",
      "2016년 4월 2일 블라디미르 푸틴 러시아 대통령의 정적 중 한 명으로 꼽혀왔던 미하일 카시야노프 인민자유당 당수가 개인비서인 나탈리아 펠레바인과 성관계를 갖는 동영상이 방송을 통해 적나라하게 공개됐다. 두 사람이 나눈 민망한 성적 대화들뿐만 아니라 분열된 야권의 다른 인사들에 대한 독설이 방송을 타고 나왔다. \n",
      "\n",
      "2016년 10월 19일, 고영태가 범삼성가이지만 삼성그룹과 대립중인 JTBC 방송사에 최초로 박근혜-최순실 게이트를 폭로했다. 고영태의 폭로로 박근혜가 탄핵, 구속되었다. 삼성그룹 이재용 부회장도 구속되었으며, 홍석현 JTBC 회장이 삼성그룹 경영권을 빼앗을 것이란 보도도 있었다. 한편, 고영태 대해 '남창' 이라며 콤프로마트 공작을 벌인 탄핵반대 모임과 일부 단체들의 배후에 대해서는 알려진 바는 없다.\n",
      "\n",
      "도널드 트럼프 대통령이 러시아 FSB의 콤프로마트 미인계 공작에 걸려 꼭두각시가 된 것은 아닌가 하는 의혹이 있다. 미스 유니버스 조직위원회를 소유하고 있는 도널드 트럼프가 2013년 러시아 모스크바에서 열린 미스 유니버스 대회를 위해 5성급 호텔인 모스크바 리츠 칼튼 호텔에 묵었다. 여기서 트럼프는 많은 미녀들과 섹스 파티를 했고, 이를 FSB가 동영상을 촬영해, 이 동영상으로 협박을 하며 트럼프를 뒤에서 조종하고 있다는 의혹이다. \n",
      "\n",
      "2013년 중국계 미국인으로서는 처음으로 중국 베이징 주재 미국 대사를 맡았다가 2013년 돌연 사의를 표했던 게리 로크(63)가 2015년 이혼했다. 베이징의 여성 사업가와 불륜을 저질렀고, 이 여성이 중국 정보부 요원이며, 콤프로마트 미인계 공작에 걸린 것으로 보도되었다. \n",
      "\n",
      "2016년 10월 네덜란드 정부가 론 켈러 베이징 주재 대사를 긴급 소환해 직무를 정지시켰다. 켈러 대사는 대사관 직원으로 채용된 중국 여성과 비밀리에 연인 관계를 맺어왔다. 중국은 과거부터 정보를 수집하기 위해 여성 간첩을 활용하는 미인계(honey trap)에 능하다는 말을 들어왔다.  요즘의 미인계는 반드시 동영상을 촬영하며, 동영상을 폭로한다고 협박하면, 대부분은 꼭두각시가 된다.\n",
      "\n",
      "Content: 2020년 1월을 기준으로 크로스링스는 전체 선로의 절반을 설치하였다. \n",
      "\n",
      "2020년 2월, 메트로링스는 이 노선이 기존 목표인 2021년 9월 대신 2022년에 개통할 것으로 보인다고 밝혔다.  메트로링스는 건설사인 크로스링스에 2억 3700만 달러의 합의금을 건네주었음에도 불구하고  메트로링스는 공사 지연의 원인으로 크로스링스가 공사를 9개월 늦게 시작하였고 설계 일부를 마무리하는 데 늦어졌고, 그와 동시에 에글린턴역에 영 지하철이 지어진 1950년대에 콘크리트 안에 매립된 배관이 발견되어서 굴착이 지연되었다고 밝혔다.  메트로링스의 비공개 문건에 따르면 경전철 공사 예산이 3억 3천만 달러 초과되었을 것으로 보인다\n",
      "\n",
      "2020년 3월 9일, 주정부는 경전철 공사로 침체된 상권을 도와주기 위해 에글린턴 애비뉴의 자영업자들에게 300만 달러를 지원할 것이라고 밝혔다. 교통부와 메트로링스는 또한 경전철의 부분 개통도 검토하였다. \n",
      "\n",
      "한편 크로스링스는 3월에 파머시역에 처음으로 경전철 승객 대기 공간을 설치하였다고 밝혔다. 이 시설은 사전에 만들어졌고 경전철 승강장 위에 설치해 고정하였다.  또한 크로스링스는 3월에 듀플렉스 애비뉴에 있던 굴착기 데니스와 레아가 임무를 마치고 지상으로 나왔다 2020년 4월 15일, 경전철 서부 터널 굴착기인 돈과 험버 또한 지상으로 나왔다. \n",
      "\n",
      "2020년 5월 4일, 메트로링스는 5호선 터널에 경전철 차량의 첫 시험 운행을 에글린턴 차량기지에서 킬스데일역까지 처음으로 완료하였다. 이 시험 운행은 이간거리, 선로 작업, 가공전력선과 통신을 확인하기 위해 이루어졌으며 터널을 최대 시속 25km로 오갔다.\n",
      "\n",
      "Content: 1939년 경주시 안강읍 신대리의 청안 이씨 충간공파의 시조 이한번(李漢蕃) 자손으로 태어나  안강제일초등학교, 경주중학교, 경주고등학교(7회)와 서울대학교 법학과를 졸업하고 제13회 고등고시 사법과에 합격하여 판사에 임용된 이정락은 서울지방법원 남부지원장과 서울가정법원. 서울형사지방법원 등에서 법원장을 역임하였다. 1993년 서울형사지방법원에서의 법원장을 마지막으로 30여년 간의 법관 생활을 마치고 변호사 개업을 하여 대한변호사협회 산하 법률구조재단의 초대 이사장을 맡아 법률 구조에 있어 기틀을 닦았다.  서울형사지방법원 법원장으로 있던 1992년에 법원에서 컴퓨터 속기 설명회를 가져 매우 좋은 평가를 받아 법원에 컴퓨터 속기가 도입되었다.  이정락은 변호사 사무소를 개업하면서  공평과 정의로 사회 고통 치유하는 힐링 변호사를 내걸고 있다. \n",
      "\n",
      "서울고등법원에서 재직할 때 이정락은 전두환 정부의 대표적인 공안 사건이었던 아람회 사건의 항소심 재판장을 맡아 \"'아람회의 목적이 구체적으로 특정되었다'고 볼 수 없을 뿐만 아니라 이 모임이 정부를 전복하겠다는 목적을 갖고 있었다고 단정할 증거가 없다\"는 이유로 국가보안법상 반국가단체 구성과 반국가단체 구성원과의 회합·통신·허위사실유포에 대해 무죄를 인정하면서 최고 징역 10년이었던 형량을 징역6년에서 집행유예 까지 낮춰 선고했으나 무죄 취지 판단에 대해 대법원에서 파기되었다. 또 진도 가족 간첩단 사건으로 1심에서 사형이 선고된 피고인에 대해 무기징역을 선고하면서 혐의를 모두 인정했다. \n",
      "\n",
      "노태우 비자금 사건에서 김우중 대우그룹 회장 변호사를 하면서 \"금전의 제공시기가 연말연시나 선거철등이고 제공장소도 공개된 장소였다.또 자금 제공의 목적도 통치자금으로 쓰도록 성금으로 전달한 것이니 만큼 대가성이 없었다\"며 \"뇌물공여가 인정되더라도 대우그룹이 아닌 시대의 관행이 뇌물 공여죄로 처벌받아야 한다.고 말했다. \n",
      "\n",
      "이정락은 대통령 후보였던 이회창의 법률지원단장을 맡으면서 후보의 대표적인 사조직으로 꼽힌 부국팀 회장을 맡았으며  이로 인하여 대기업으로부터 거액을 불법 후원받은 등의 이유로 검찰 조사를 받았다.\n",
      "\n",
      "사단법인 과학사랑∙나라사랑 관계자로부터 \"허쉬바흐 박사가 한국의 시골 초등학교를 보고 싶어한다\"는 말을 전해 들은 이정락은 2002년 10월23일에 허쉬바흐 박사와 함께 자신이 총동창회장으로 있는 모교인 안강제일초등학교를 방문하여 \"기왕이면 우리 고장, 우리 모교를 방문하는 것이 영광스러울 것 같아 추천했다”고 말하면서 \"허쉬바흐 박사는 외국의 어느 손님보다 귀한 손님이다\"며 \"앞으로 모교에서 노벨상 수상자가 탄생한다면 허쉬바흐 박사의 방문이 더욱 값지게 될 것”이라고 했다. \n",
      "\n",
      "안강읍 안강찰토마토작목반원을 비롯해 현곡면 현곡토마토작목반 회원들과 그 가족들이 불량씨앗을 판매한 종묘상을 상대로 소송을 하다 서울지방법원에서 패소한 것을 서울고등법원에 항소하는 과정에서 이정락 변호사가 \"고향주민들의 어려움을 덜어주겠다\"며 자청해서 무료변론 끝에 고법에서 승소를 했다. \n",
      "\n",
      "보수성향 변호사의 모임으로 알려진 헌변 회원이자 대한변호사협회 법률구조재단 이사장을 맡고 있던 2007년에  \"법률구조사업이야 말로 이제 우리 변호사들의 책임이고 의무\"라며 \"변호사의 노블리스 오블리쥬는 바로 법률구조운동에 헌신적으로 참여하고 실천하는 길\"이라고 말했다. \n",
      "\n",
      "중도 성향 변호사 모임을 자처하며 2005년에 창립된 '시민과 함께하는 변호사들이 \"법원 내 이념 편향적인 판사들의 활동에 대해 우려해왔고, 이른바 '튀는 판결'을 감시해야 한다\"면서 창립 5주년 기념 세미나에 참석했다. \n",
      "\n",
      "경주고도보존회와 재향 경우회 회장을 맡은 이정락은 2012년 3월 29일에 모교인 경주고등학교 1,2학년을 대상으로 미국 시인 ‘프로스트’의 ‘눈 오는 저녁 숲가에 서서’의 시 한구절인 ‘miles to go before I sleep, 잠들기 전에 가야할 길이 있다’를 인용하여 학생들이 미래에 가야하는 길에 대한 자세와 마음가짐에 관하여 3C(confidence, Concentration, Consistency)으로 강연하였다. \n",
      "\n",
      "박근혜 정부의 문화예술계 지원배제 명단 '블랙리스트'를 작성·집행을 주도한 혐의를 받은 김기춘과 조윤선의 변호인단으로 선임된 이정락은 항소심 결심 공판에서  \"피고인들이 순차적으로 공모해 청와대 정무수석실 주도로 태스크포스(TF)를 구성했다\"는 특검 주장에 대해 \"어떤 명목이든지 대통령 비서실장으로 실패한 대통령과 함께 처벌해야 한다는 억지 논리는 지금의 법치주의 하에서 도저히 허용될 수 없다\"면서 \"특검이 제시한 증거는 어느 공소사실에 부합하는지 밝히지 못하고 순차 공모부분 역시 추상적이고 애매하다\"고 반박하며 피고인들에 대해 관대한 처벌을 요청했다.\n",
      "\n",
      "Content: 브라이언 엡스타인이 1967년 8월 27일 사망한 이후, 비틀즈는 창작열에 불타올랐다. 그러나 그 전 밴드는 며칠 간을 충격 속에 보냈고, 폴 매카트니는 그들의 비탄을 통제할 한 가지 묘수를 떠올린다. 바로 스튜디오로 돌아가는 것이었다. 9월 5일부터 시작된 작업에서, 존 레논은 자신이 졸업한 고등학교의 학생이 비틀즈의 가사에서 숨겨진 의미를 찾는 공부를 하고 있다는 편지에서 영감을 받은 새 일렉트릭 곡을 가져왔다. 레논은 〈I Am the Walrus〉의 솔로 어쿠스틱 버전을 연주했고, 당시 상황을 엔지니어 제프 에머릭은 이렇게 회상했다. \"모두가 어리둥절 해 있었다. 멜로디는 크게 두 개로 구성돼 있었는데, 가사는 꽤나 넌센스해 보였다.\"  등장하는 해마는 루이스 캐럴의 시 〈해마와 목수〉 (《거울 나라의 앨리스》에서 등장)에서 가져왔다. 레논은 시에서 등장하는 해마가 실은 악당이었다는 사실을 뒤늦게 알고는 경악했다.p=185\n",
      "\n",
      "조지 마틴은 \"날 제외하고 너희끼리 뭘 하려는 심산이지?\"라고 말했다. 결국에는 모두가 트랙에 참여했다. 레논은 단순한 일렉트릭 형태의 반주를 붙였고, 매카트니는 스타가 박자를 지킬 수 있도록 탬버린을 흔들었다. 매카트니의 이러한 근면성실함은 밴드를 집중할 수 있도록 도왔고, 에머릭은 이 순간을 회상하며 \"폴의 가장 멋진 순간\"이라고 했다. 트랙은 후기 제작에서 강렬하며, 멍한 느낌을 가질 수 있었다. 초기에 받은 충격에도 아랑곳 않고, 마틴은 능수능란한 솜씨로 오케스트라 편곡을 작곡해 마치 어지럼증과 같은 느낌을 부여했다. 마치 달에서 온 사운드를 원했던 레논은, 자신의 목소리를 최대한 찌그려뜨려 달라는 부탁을 했다\n",
      "\n",
      "레논은 \"낱말들은 사실 별 의미가 없다\"며 \"사람들이 너무나 많은 추측을 했고, 그건 하나같이 말도 안 되는 것이었다. 'I am the Eggman?'의 진정한 뜻이 뭐냐고? 나도 잘 모른다. 그건 푸딩 그릇이 될 수도 있었다.\"고 말했다. 가사는 그 내부에 수많은 말장난이 포함돼 있다. \"Semolina pilchard\"는 런던의 마약계 경찰인 노먼 필처에 대한 언급이다. 그는 믹 재거나 키스 리처즈와 같은 록 스타를 잡아들이곤 했다. 그리고 \"The Eggman\"은 캐롤의 험티 덤티와 레논이 에릭 버든으로부터 들은, 그 무렵 한 소녀가 애니멀스의 프론트맨과 섹스 도중 그 위에서 달걀을 깨뜨렸다는 얘기에 기원이 있다. 이듬해의 화이트 앨범에서, 레논은 〈Glass Onion〉에 \"해마는 폴이다(The walrus was Paul)\"라는 곡의 언급을 했다. 이는 엡스타인의 죽음 이후 그룹을 규합할 수 있도록 도운 매카트니의 대한 그만의 감사 방법이었다\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#리트리버출력 확인\n",
    "query = \"처음으로 부실 경영인에 대한 보상 선고를 받은 회사는?\"\n",
    "ensemble_result = retriever.invoke(query)\n",
    "bm25_result = kiwi_bm25.invoke(query)\n",
    "\n",
    "# 가져온 문서를 출력합니다.\n",
    "print(\"[Ensemble Retriever]\")\n",
    "for doc in ensemble_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print(\"[BM25 Retriever]\")\n",
    "for doc in bm25_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from datasets) (4.66.3)\n",
      "Requirement already satisfied: xxhash in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from datasets) (0.25.2)\n",
      "Requirement already satisfied: packaging in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from aiohttp->datasets) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /data/ephemeral/home/minjun/.conda/envs/minjun/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "\n",
    "#dataset = load_from_disk(\"/data/ephemeral/home/donghyeok/data/train_dataset/\")\n",
    "dataset = load_from_disk(\"/data/ephemeral/home/donghyeok/data/test_dataset/\")\n",
    "\n",
    "#print(dataset['validation'])\n",
    "data_val = dataset['validation']\n",
    "df_val  = pd.DataFrame(data_val )\n",
    "\n",
    "\n",
    "##########validation 용도 \n",
    "# df_q = df_val[['question', 'id']]\n",
    "# print(df_q)\n",
    "# df_a = df_val['answers']\n",
    "# answer = []\n",
    "# for text in df_a :\n",
    "#     answer.append(text['text'])\n",
    "# df_new_a = pd.DataFrame({'text': answer}) # [글자]\n",
    "####################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import load_prompt\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.example_selectors import (\n",
    "    MaxMarginalRelevanceExampleSelector,\n",
    "    SemanticSimilarityExampleSelector,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "\n",
    "############################ fewshot prompt 예제 ############################\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"스티브 잡스와 아인슈타인 중 누가 더 오래 살았나요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 스티브 잡스는 몇 살에 사망했나요?\n",
    "중간 답변: 스티브 잡스는 56세에 사망했습니다.\n",
    "추가 질문: 아인슈타인은 몇 살에 사망했나요?\n",
    "중간 답변: 아인슈타인은 76세에 사망했습니다.\n",
    "최종 답변은: 아인슈타인\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"네이버의 창립자는 언제 태어났나요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 네이버의 창립자는 누구인가요?\n",
    "중간 답변: 네이버는 이해진에 의해 창립되었습니다.\n",
    "추가 질문: 이해진은 언제 태어났나요?\n",
    "중간 답변: 이해진은 1967년 6월 22일에 태어났습니다.\n",
    "최종 답변은: 1967년 6월 22일\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"율곡 이이의 어머니가 태어난 해의 통치하던 왕은 누구인가요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 율곡 이이의 어머니는 누구인가요?\n",
    "중간 답변: 율곡 이이의 어머니는 신사임당입니다.\n",
    "추가 질문: 신사임당은 언제 태어났나요?\n",
    "중간 답변: 신사임당은 1504년에 태어났습니다.\n",
    "추가 질문: 1504년에 조선을 통치한 왕은 누구인가요?\n",
    "중간 답변: 1504년에 조선을 통치한 왕은 연산군입니다.\n",
    "최종 답변은: 연산군\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"올드보이와 기생충의 감독이 같은 나라 출신인가요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 올드보이의 감독은 누구인가요?\n",
    "중간 답변: 올드보이의 감독은 박찬욱입니다.\n",
    "추가 질문: 박찬욱은 어느 나라 출신인가요?\n",
    "중간 답변: 박찬욱은 대한민국 출신입니다.\n",
    "추가 질문: 기생충의 감독은 누구인가요?\n",
    "중간 답변: 기생충의 감독은 봉준호입니다.\n",
    "추가 질문: 봉준호는 어느 나라 출신인가요?\n",
    "중간 답변: 봉준호는 대한민국 출신입니다.\n",
    "최종 답변은: 예\n",
    "\"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # 여기에는 선택 가능한 예시 목록이 있습니다.\n",
    "    examples,\n",
    "    # 여기에는 의미적 유사성을 측정하는 데 사용되는 임베딩을 생성하는 임베딩 클래스가 있습니다.\n",
    "    OpenAIEmbeddings(),\n",
    "    # 여기에는 임베딩을 저장하고 유사성 검색을 수행하는 데 사용되는 VectorStore 클래스가 있습니다.\n",
    "    Chroma,\n",
    "    # 이것은 생성할 예시의 수입니다.\n",
    "    k=1,\n",
    ")\n",
    "\n",
    "question = \"Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?\"\n",
    "\n",
    "selected_examples = example_selector.select_examples({\"question\": question})\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"],\n",
    "    template=\"\"\"\n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question:\\n{question}\\nAnswer:\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "\n",
    "############################ generator 및 chain 생성 ############################\n",
    "model = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    model_name=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m처음으로 부실 경영인에 대한 보상 선고를 받은 회사는?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# invoke를 사용해 입력을 체인에 넘김\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/minjun/.conda/envs/minjun/lib/python3.10/site-packages/langchain_core/runnables/base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   3021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3024\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/minjun/.conda/envs/minjun/lib/python3.10/site-packages/langchain_core/runnables/base.py:3727\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3722\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3723\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3724\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[1;32m   3725\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3726\u001b[0m         ]\n\u001b[0;32m-> 3727\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3728\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/minjun/.conda/envs/minjun/lib/python3.10/site-packages/langchain_core/runnables/base.py:3727\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3722\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3723\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3724\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[1;32m   3725\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3726\u001b[0m         ]\n\u001b[0;32m-> 3727\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3728\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/minjun/.conda/envs/minjun/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/minjun/.conda/envs/minjun/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/minjun/.conda/envs/minjun/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/minjun/.conda/envs/minjun/lib/python3.10/site-packages/langchain_core/runnables/base.py:3711\u001b[0m, in \u001b[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001b[0;34m(step, input, config, key)\u001b[0m\n\u001b[1;32m   3709\u001b[0m context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   3710\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m-> 3711\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3713\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3715\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/minjun/.conda/envs/minjun/lib/python3.10/site-packages/langchain/retrievers/ensemble.py:118\u001b[0m, in \u001b[0;36mEnsembleRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    117\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[1;32m    121\u001b[0m         result,\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    123\u001b[0m     )\n",
      "File \u001b[0;32m~/minjun/.conda/envs/minjun/lib/python3.10/site-packages/langchain/retrievers/ensemble.py:115\u001b[0m, in \u001b[0;36mEnsembleRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_retriever_start(\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    111\u001b[0m     name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(),\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    113\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrank_fusion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    117\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n",
      "File \u001b[0;32m~/minjun/.conda/envs/minjun/lib/python3.10/site-packages/langchain/retrievers/ensemble.py:222\u001b[0m, in \u001b[0;36mEnsembleRetriever.rank_fusion\u001b[0;34m(self, query, run_manager, config)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03mRetrieve the results of the retrievers and use rank_fusion_func to get\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03mthe final result.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    A list of reranked documents.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# Get the results of all retrievers.\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m retriever_docs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    223\u001b[0m     retriever\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m    224\u001b[0m         query,\n\u001b[1;32m    225\u001b[0m         patch_config(\n\u001b[1;32m    226\u001b[0m             config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretriever_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    227\u001b[0m         ),\n\u001b[1;32m    228\u001b[0m     )\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, retriever \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrievers)\n\u001b[1;32m    230\u001b[0m ]\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# Enforce that retrieved docs are Documents for each list in retriever_docs\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(retriever_docs)):\n",
      "File \u001b[0;32m~/minjun/.conda/envs/minjun/lib/python3.10/site-packages/langchain/retrievers/ensemble.py:223\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03mRetrieve the results of the retrievers and use rank_fusion_func to get\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03mthe final result.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    A list of reranked documents.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# Get the results of all retrievers.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m retriever_docs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 223\u001b[0m     \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretriever_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, retriever \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrievers)\n\u001b[1;32m    230\u001b[0m ]\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# Enforce that retrieved docs are Documents for each list in retriever_docs\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(retriever_docs)):\n",
      "File \u001b[0;32m~/minjun/.conda/envs/minjun/lib/python3.10/site-packages/langchain_core/retrievers.py:254\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    253\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[1;32m    257\u001b[0m         result,\n\u001b[1;32m    258\u001b[0m     )\n",
      "File \u001b[0;32m~/minjun/.conda/envs/minjun/lib/python3.10/site-packages/langchain_core/retrievers.py:247\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 247\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m~/donghyeok/helper.py:185\u001b[0m, in \u001b[0;36mKiwiBM25Retriever._get_relevant_documents\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_relevant_documents\u001b[39m(\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[1;32m    184\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m--> 185\u001b[0m     processed_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     return_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_top_n(processed_query, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocs, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk)\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_docs\n",
      "File \u001b[0;32m~/donghyeok/helper.py:82\u001b[0m, in \u001b[0;36mbert_preprocessing_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbert_preprocessing_func\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbert_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m~/minjun/.conda/envs/minjun/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:411\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.tokenize\u001b[0;34m(self, text, pair, add_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, pair: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, add_special_tokens: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtokens()\n",
      "File \u001b[0;32m~/minjun/.conda/envs/minjun/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3202\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3192\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3193\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3194\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3195\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3199\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3200\u001b[0m )\n\u001b[0;32m-> 3202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3205\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3221\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3222\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3223\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/minjun/.conda/envs/minjun/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:603\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    581\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    601\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    602\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 603\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/minjun/.conda/envs/minjun/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:529\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 529\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    541\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    543\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    553\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "question = \"처음으로 부실 경영인에 대한 보상 선고를 받은 회사는?\"\n",
    "\n",
    "# invoke를 사용해 입력을 체인에 넘김\n",
    "result = chain.invoke({\"question\": question})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test에 대한 답 generate \n",
    "import json\n",
    "predictions = {}\n",
    "for index, row in df_val.iterrows():\n",
    "    result = chain.invoke({\"question\": row['question']})\n",
    "    predictions[row['id']] = result\n",
    "\n",
    "# 제출용 json 생성\n",
    "with open('predictions.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(predictions, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
