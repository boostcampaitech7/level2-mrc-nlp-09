{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation - Distant Supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /data/ephemeral/home/miniconda3/lib/python3.12/site-packages (from scikit-learn) (2.1.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /data/ephemeral/home/miniconda3/lib/python3.12/site-packages (4.66.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from transformers import DPRQuestionEncoder, DPRContextEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. Train 데이터 로드\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"/data/ephemeral/home/jeongeun/data/raw/train_dataset\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "train_data=pd.DataFrame(train_dataset)\n",
    "\n",
    "# 2. DPR 모델 로드\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "\n",
    "# 3. 위키피디아 문서 로드\n",
    "wikipedia_data_path = '/data/ephemeral/home/jeongeun/data/raw/wikipedia_documents.json'\n",
    "with open(wikipedia_data_path, 'r') as f:\n",
    "    wikipedia_data = json.load(f)\n",
    "\n",
    "# Helper function to encode questions and contexts\n",
    "def encode_question(question, max_length=512):\n",
    "    inputs = question_tokenizer(question, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "    question_emb = question_encoder(**inputs).pooler_output\n",
    "    return question_emb\n",
    "\n",
    "def encode_context(context, max_length=512):\n",
    "    inputs = context_tokenizer(context, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "    context_emb = context_encoder(**inputs).pooler_output\n",
    "    return context_emb\n",
    "\n",
    "# 4. 유사도 높은 문서 찾기\n",
    "def find_similar_document(question, wikipedia_data):\n",
    "    question_emb = encode_question(question)\n",
    "    \n",
    "    similarities = []\n",
    "    doc_ids = []\n",
    "    \n",
    "    # 각 문서에 대해 첫 문단의 embedding을 계산하고 유사도 비교\n",
    "    for doc_id, document in wikipedia_data.items():\n",
    "        first_paragraph = document['text'].split('\\n')[0]  # 첫 문단 가져오기\n",
    "        context_emb = encode_context(first_paragraph)\n",
    "        similarity = cosine_similarity(question_emb.detach().numpy(), context_emb.detach().numpy())\n",
    "        similarities.append(similarity[0][0])\n",
    "        doc_ids.append(doc_id)\n",
    "    \n",
    "    # 유사도가 가장 높은 문서 선택\n",
    "    best_match_idx = similarities.index(max(similarities))\n",
    "    return doc_ids[best_match_idx], wikipedia_data[doc_ids[best_match_idx]]\n",
    "\n",
    "# 5. 증강된 데이터셋 생성\n",
    "from tqdm import tqdm\n",
    "\n",
    "augmented_data = []\n",
    "\n",
    "for idx, row in tqdm(train_data.iterrows(), total=train_data.shape[0], desc=\"Augmenting Data\"):\n",
    "    question = row['question']\n",
    "    current_doc_id = row['document_id']\n",
    "    \n",
    "    # DPR로 유사도가 높은 문서 찾기\n",
    "    new_doc_id, similar_doc = find_similar_document(question, wikipedia_data)\n",
    "    \n",
    "    # 만약 다른 문서라면 해당 문서에서 답이 있는 단락까지 추출해 context로 사용\n",
    "    if new_doc_id != current_doc_id:\n",
    "        # 답이 포함된 단락 찾기 (단순히 answer 문자열이 있는지 확인)\n",
    "        answer = row['answer']\n",
    "        context = \"\"\n",
    "        for paragraph in similar_doc['text'].split('\\n'):\n",
    "            context += paragraph + \"\\n\"\n",
    "            if answer in paragraph:\n",
    "                break\n",
    "        \n",
    "        # 기존 train 데이터에 행 추가 (새로운 문서와 context 사용)\n",
    "        new_row = row.copy()\n",
    "        new_row['document_id'] = new_doc_id\n",
    "        new_row['context'] = context\n",
    "        augmented_data.append(new_row)\n",
    "\n",
    "# 6. 증강된 데이터 저장\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "augmented_data = pd.DataFrame(augmented_data)\n",
    "augmented_data = pd.concat([train_data, augmented_data], ignore_index=True)\n",
    "train_dataset = Dataset.from_pandas(augmented_data)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset\n",
    "})\n",
    "\n",
    "dataset_dict.save_to_disk('/data/ephemeral/home/jeongeun/data/preprocessed/train_dataset_aug_DS')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
